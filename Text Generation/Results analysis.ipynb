{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "load_path = \"/scratch/koehler.ale/FinalMIMICDF.csv\"\n",
    "\n",
    "\n",
    "FinalMIMICDF = load_dataframe(load_path)\n",
    "FinalMIMICDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_comparison_data(term_analysis: pd.DataFrame, bins: int = 20) -> Dict:\n",
    "    \"\"\"\n",
    "    Prepare TF-IDF comparison data focusing on model adaptation.\n",
    "    \n",
    "    Args:\n",
    "        term_analysis: DataFrame with TF-IDF scores\n",
    "        bins: Number of bins for grouping TF-IDF scores\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with scatter plot data for trained and untrained models\n",
    "    \"\"\"\n",
    "    def create_binned_data(scores: np.ndarray) -> List[Dict[str, float]]:\n",
    "        # Create histogram data\n",
    "        hist, bin_edges = np.histogram(scores, bins=bins, range=(0, 1))\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        \n",
    "        # Convert to scatter format\n",
    "        scatter_data = []\n",
    "        for count, tfidf in zip(hist, bin_centers):\n",
    "            if count > 0:  # Only include bins with terms\n",
    "                scatter_data.append({\n",
    "                    'tfidf': float(tfidf),\n",
    "                    'count': int(count)\n",
    "                })\n",
    "        return scatter_data\n",
    "\n",
    "    # Calculate relative TF-IDF scores (how well each model matches reference patterns)\n",
    "    trained_similarity = 1 - abs(term_analysis['trained_mean'] - term_analysis['reference_mean'])\n",
    "    untrained_similarity = 1 - abs(term_analysis['untrained_mean'] - term_analysis['reference_mean'])\n",
    "    \n",
    "    scatter_data = {\n",
    "        'trained': create_binned_data(trained_similarity),\n",
    "        'untrained': create_binned_data(untrained_similarity)\n",
    "    }\n",
    "    \n",
    "    # Add summary statistics\n",
    "    stats = {\n",
    "        'trained': {\n",
    "            'mean_similarity': float(trained_similarity.mean()),\n",
    "            'std_similarity': float(trained_similarity.std()),\n",
    "            'median_similarity': float(trained_similarity.median())\n",
    "        },\n",
    "        'untrained': {\n",
    "            'mean_similarity': float(untrained_similarity.mean()),\n",
    "            'std_similarity': float(untrained_similarity.std()),\n",
    "            'median_similarity': float(untrained_similarity.median())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'scatter_data': scatter_data,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "def analyze_model_adaptation(\n",
    "    trained_summaries: List[str],\n",
    "    untrained_summaries: List[str],\n",
    "    reference_summaries: List[str]\n",
    ") -> Tuple[Dict, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Analyze how well models have adapted to reference patterns.\n",
    "    \"\"\"\n",
    "    # Run basic analysis first\n",
    "    report, term_analysis = analyze_summaries(\n",
    "        trained_summaries,\n",
    "        untrained_summaries,\n",
    "        reference_summaries\n",
    "    )\n",
    "    \n",
    "    # Prepare comparison data\n",
    "    comparison_data = prepare_model_comparison_data(term_analysis)\n",
    "    \n",
    "    # Add adaptation metrics to report\n",
    "    report['adaptation_metrics'] = {\n",
    "        'improvement_ratio': (\n",
    "            comparison_data['stats']['trained']['mean_similarity'] /\n",
    "            comparison_data['stats']['untrained']['mean_similarity']\n",
    "        ),\n",
    "        'absolute_improvement': (\n",
    "            comparison_data['stats']['trained']['mean_similarity'] -\n",
    "            comparison_data['stats']['untrained']['mean_similarity']\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return report, comparison_data\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    report, comparison_data = analyze_model_adaptation(\n",
    "        FinalMIMICDF['Trained_Summary'].tolist(),\n",
    "        FinalMIMICDF['Untrained_Summary'].tolist(),\n",
    "        FinalMIMICDF['Real_Summary'].tolist()\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Adaptation Analysis:\")\n",
    "    print(f\"Improvement ratio: {report['adaptation_metrics']['improvement_ratio']:.2f}x\")\n",
    "    print(f\"Absolute improvement: {report['adaptation_metrics']['absolute_improvement']:.3f}\")\n",
    "    \n",
    "    return report, comparison_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFDF = calculate_tfidf_scores(sample_df)\n",
    "TFDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import statistics as stats \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def analyze_and_visualize_tfidf(comparison_df, output_dir='./plots'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations and statistical analysis of TF-IDF scores.\n",
    "    \n",
    "    Args:\n",
    "        comparison_df: DataFrame from calculate_tfidf_scores\n",
    "        output_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Calculate additional statistics\n",
    "    score_diff = comparison_df['Score_Difference']\n",
    "    similarity_ratio = comparison_df['Similarity_Ratio']\n",
    "    \n",
    "    stats_results = {\n",
    "        'std_difference': np.std(score_diff),\n",
    "        'std_similarity': np.std(similarity_ratio),\n",
    "        'mean_difference': np.mean(score_diff),\n",
    "        'mean_similarity': np.mean(similarity_ratio)\n",
    "    }\n",
    "    \n",
    "    # Top 5 Words with Significant TF-IDF Differences\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    significant_diff = comparison_df.nlargest(5, 'Score_Difference')  # Changed to top 5\n",
    "    sns.barplot(data=significant_diff, y='Word', x='Score_Difference', \n",
    "        palette=['purple', 'blue'])\n",
    "    plt.title('Top Words with Significant TF-IDF Differences')\n",
    "    plt.xlabel('Score Difference')\n",
    "    plt.legend(\n",
    "        handles=[\n",
    "            plt.Line2D([0], [0], color='purple', lw=4, label='Generated'), \n",
    "            plt.Line2D([0], [0], color='blue', lw=4, label='Real')    \n",
    "        ],\n",
    "        loc='best',  \n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Top 5 Words with Highest Similarity\n",
    "    plt.figure(figsize=(6,4))\n",
    "    most_similar = comparison_df.nlargest(5, 'Similarity_Ratio')  # Changed to top 5\n",
    "    sns.barplot(data=most_similar, y='Word', x='Similarity_Ratio')\n",
    "    plt.title('Top Words with Highest Similarity')\n",
    "    plt.xlabel('Similarity Ratio')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return stats_results\n",
    "\n",
    "def print_analysis_report(stats_results):\n",
    "    \"\"\"\n",
    "    Print a formatted report of the statistical analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\nTF-IDF Analysis Statistical Report\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Standard Deviation of Differences: {stats_results['std_difference']:.4f}\")\n",
    "    print(f\"Standard Deviation of Similarities: {stats_results['std_similarity']:.4f}\")\n",
    "    print(f\"Mean Difference: {stats_results['mean_difference']:.4f}\")\n",
    "    print(f\"Mean Similarity: {stats_results['mean_similarity']:.4f}\")\n",
    "\n",
    "\n",
    "stats_results = analyze_and_visualize_tfidf(TFDF)\n",
    "\n",
    "print_analysis_report(stats_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_tfidf_histograms(comparison_df, output_dir='./plots'):\n",
    "    \"\"\"\n",
    "    Create precise histogram visualizations of TF-IDF score differences and similarity ratios.\n",
    "    \n",
    "    Args:\n",
    "        comparison_df: DataFrame containing TF-IDF comparison results\n",
    "        output_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    # Set style\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 4))\n",
    "    \n",
    "    # Plot Score Differences Histogram\n",
    "    ax1.hist(comparison_df['Score_Difference'], \n",
    "             bins=30,  # Adjust number of bins as needed\n",
    "             edgecolor='black',\n",
    "             alpha=0.7,\n",
    "             color='red')\n",
    "    ax1.axvline(comparison_df['Score_Difference'].mean(), \n",
    "                color='blue', \n",
    "                linestyle='dashed', \n",
    "                linewidth=2,\n",
    "                label=f\"Mean: {comparison_df['Score_Difference'].mean():.3f}\")\n",
    "    ax1.set_title('Distribution of TF-IDF Score Differences', pad=20)\n",
    "    ax1.set_xlabel('Score Difference')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add counts above bars for Score Differences\n",
    "    for patch in ax1.patches:\n",
    "        height = patch.get_height()\n",
    "        ax1.text(patch.get_x() + patch.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Plot Similarity Ratio Histogram\n",
    "    ax2.hist(comparison_df['Similarity_Ratio'],\n",
    "             bins=30,  # Adjust number of bins as needed\n",
    "             edgecolor='black',\n",
    "             alpha=0.7,\n",
    "             color='green')\n",
    "    ax2.axvline(comparison_df['Similarity_Ratio'].mean(),\n",
    "                color='blue',\n",
    "                linestyle='dashed',\n",
    "                linewidth=2,\n",
    "                label=f\"Mean: {comparison_df['Similarity_Ratio'].mean():.3f}\")\n",
    "    ax2.set_title('Distribution of Similarity Ratios', pad=20)\n",
    "    ax2.set_xlabel('Similarity Ratio')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add counts above bars for Similarity Ratios\n",
    "    for patch in ax2.patches:\n",
    "        height = patch.get_height()\n",
    "        ax2.text(patch.get_x() + patch.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nScore Difference Statistics:\")\n",
    "    print(f\"Mean: {comparison_df['Score_Difference'].mean():.3f}\")\n",
    "    print(f\"Median: {comparison_df['Score_Difference'].median():.3f}\")\n",
    "    print(f\"Std Dev: {comparison_df['Score_Difference'].std():.3f}\")\n",
    "    print(f\"Total Words: {len(comparison_df)}\")\n",
    "    \n",
    "    print(\"\\nSimilarity Ratio Statistics:\")\n",
    "    print(f\"Mean: {comparison_df['Similarity_Ratio'].mean():.3f}\")\n",
    "    print(f\"Median: {comparison_df['Similarity_Ratio'].median():.3f}\")\n",
    "    print(f\"Std Dev: {comparison_df['Similarity_Ratio'].std():.3f}\")\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# After calculating TF-IDF scores:\n",
    "plot_tfidf_histograms(TFDF, output_dir='./plots')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_semantic_similarities(df, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Compute semantic similarities between generated, untrained, and real summaries.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Generated_Summary', 'Real_Summary', and 'Untrained_Summary' columns\n",
    "        model_name: Name of the SBERT model to use\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with original columns plus semantic similarity scores\n",
    "    \"\"\"\n",
    "    # Load model and compute embeddings\n",
    "    model = SentenceTransformer(model_name)\n",
    "    generated_embeddings = model.encode(df['Generated_Summary'].tolist())\n",
    "    untrained_embeddings = model.encode(df['Untrained_Summary'].tolist())\n",
    "    real_embeddings = model.encode(df['Real_Summary'].tolist())\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    gen_to_real_similarities = [\n",
    "        cosine_similarity([gen_emb], [real_emb])[0][0]\n",
    "        for gen_emb, real_emb in zip(generated_embeddings, real_embeddings)\n",
    "    ]\n",
    "    untrained_to_real_similarities = [\n",
    "        cosine_similarity([untrained_emb], [real_emb])[0][0]\n",
    "        for untrained_emb, real_emb in zip(untrained_embeddings, real_embeddings)\n",
    "    ]\n",
    "    \n",
    "    # Add similarities to the dataframe\n",
    "    df_with_scores = df.copy()\n",
    "    df_with_scores['Generated_to_Real_Similarity'] = gen_to_real_similarities\n",
    "    df_with_scores['Untrained_to_Real_Similarity'] = untrained_to_real_similarities\n",
    "    \n",
    "    return df_with_scores\n",
    "\n",
    "# Example DataFrame\n",
    "sample_df = pd.DataFrame({\n",
    "    'Generated_Summary': [\"This is a fine-tuned summary.\", \"Another fine-tuned result.\"],\n",
    "    'Real_Summary': [\"This is the real summary.\", \"Another real summary here.\"],\n",
    "    'Untrained_Summary': [\"This is an untrained summary.\", \"Another untrained output.\"]\n",
    "})\n",
    "\n",
    "# Compute semantic similarities\n",
    "Semanticdf = compute_semantic_similarities(sample_df)\n",
    "\n",
    "# Calculate improvement from fine-tuning\n",
    "Semanticdf['Improvement'] = (\n",
    "    Semanticdf['Generated_to_Real_Similarity'] - Semanticdf['Untrained_to_Real_Similarity']\n",
    ")\n",
    "\n",
    "# Print the DataFrame with similarities and improvement\n",
    "print(Semanticdf)\n",
    "\n",
    "# Plot improvement\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(Semanticdf.index, Semanticdf['Generated_to_Real_Similarity'], label=\"Fine-Tuned Similarity\", marker='o')\n",
    "plt.plot(Semanticdf.index, Semanticdf['Untrained_to_Real_Similarity'], label=\"Untrained Similarity\", marker='x')\n",
    "plt.title(\"Cosine Similarity Improvement with Fine-Tuning\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def format_summaries_for_viz(df):\n",
    "    \"\"\"\n",
    "    Format dataframe with similarity scores into the structure needed for visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with columns for generated summary, real summary, and semantic similarity\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing most and least similar summary pairs\n",
    "    \"\"\"\n",
    "    # Sort by semantic similarity score\n",
    "    df_sorted = df.copy().sort_values('Semantic_Similarity', ascending=False)\n",
    "    \n",
    "    # Get top 3 most similar and least similar pairs\n",
    "    most_similar = df_sorted.head(3)\n",
    "    least_similar = df_sorted.tail(3)\n",
    "    \n",
    "    # Format the results for the React component\n",
    "    result = {\n",
    "        'mostSimilar': [\n",
    "            {\n",
    "                'generated': row['Generated_Summary'],\n",
    "                'real': row['Real_Summary'],\n",
    "                'similarity': row['Semantic_Similarity']\n",
    "            }\n",
    "            for _, row in most_similar.iterrows()\n",
    "        ],\n",
    "        'leastSimilar': [\n",
    "            {\n",
    "                'generated': row['Generated_Summary'],\n",
    "                'real': row['Real_Summary'],\n",
    "                'similarity': row['Semantic_Similarity']\n",
    "            }\n",
    "            for _, row in least_similar.iterrows()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Optionally save to JSON file\n",
    "    with open('similarity_results.json', 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "        \n",
    "    return result\n",
    "\n",
    "format_summaries_for_viz(Semanticdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tfidf_to_sentence(TFDF):\n",
    "    \"\"\"\n",
    "    Aggregate word-level TF-IDF scores to sentence level.\n",
    "    \n",
    "    Args:\n",
    "        TFDF: DataFrame containing word-level TF-IDF scores with 'Index' column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sentence-level TF-IDF scores\n",
    "    \"\"\"\n",
    "    # Group by sentence (Index) and calculate mean scores\n",
    "    sentence_tfidf = TFDF.groupby('Index').agg({\n",
    "        'Similarity_Ratio': 'mean',\n",
    "        'Score_Difference': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return sentence_tfidf\n",
    "\n",
    "def plot_sentence_level_comparison(SemanticDF, TFDF, RougeDF):\n",
    "    \"\"\"\n",
    "    Create visualization comparing sentence-level semantic, TF-IDF, and ROUGE similarities.\n",
    "    \n",
    "    Args:\n",
    "        SemanticDF: DataFrame with semantic similarity scores per sentence\n",
    "        TFDF: DataFrame with word-level TF-IDF scores\n",
    "        RougeDF: DataFrame with ROUGE scores per sentence\n",
    "    \"\"\"\n",
    "    # First aggregate TF-IDF scores to sentence level\n",
    "    sentence_tfidf = aggregate_tfidf_to_sentence(TFDF)\n",
    "    \n",
    "    # Sort all DataFrames by index to ensure alignment\n",
    "    SemanticDF = SemanticDF.sort_index().reset_index(drop=True)\n",
    "    sentence_tfidf = sentence_tfidf.sort_values('Index').reset_index(drop=True)\n",
    "    RougeDF = RougeDF.sort_index().reset_index(drop=True)\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot all similarity scores\n",
    "    plt.plot(range(len(SemanticDF)), \n",
    "             SemanticDF['Semantic_Similarity'],\n",
    "             'o-', color='blue', alpha=0.6,\n",
    "             label='Semantic Similarity',\n",
    "             markersize=6)\n",
    "    \n",
    "    plt.plot(range(len(sentence_tfidf)),\n",
    "             sentence_tfidf['Similarity_Ratio'], \n",
    "             's-', color='red', alpha=0.6,\n",
    "             label='Average TF-IDF Similarity',\n",
    "             markersize=6)\n",
    "    \n",
    "    plt.plot(range(len(RougeDF)),\n",
    "             RougeDF['Rouge_Score'], \n",
    "             '^-', color='green', alpha=0.6,\n",
    "             label='ROUGE Score',\n",
    "             markersize=6)\n",
    "    \n",
    "    # Add mean lines\n",
    "    plt.axhline(y=SemanticDF['Semantic_Similarity'].mean(),\n",
    "                color='blue', linestyle='--', alpha=0.5,\n",
    "                label=f'Semantic Mean: {SemanticDF[\"Semantic_Similarity\"].mean():.3f}')\n",
    "    \n",
    "    plt.axhline(y=sentence_tfidf['Similarity_Ratio'].mean(),\n",
    "                color='red', linestyle='--', alpha=0.5,\n",
    "                label=f'TF-IDF Mean: {sentence_tfidf[\"Similarity_Ratio\"].mean():.3f}')\n",
    "    \n",
    "    plt.axhline(y=RougeDF['Rouge_Score'].mean(),\n",
    "                color='green', linestyle='--', alpha=0.5,\n",
    "                label=f'ROUGE Mean: {RougeDF[\"Rouge_Score\"].mean():.3f}')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Sentence-Level Comparison of Semantic, TF-IDF, and ROUGE Similarity Scores', pad=20)\n",
    "    plt.xlabel('Sentence Index')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSentence-Level Summary Statistics:\")\n",
    "    print(\"\\nSemantic Similarity:\")\n",
    "    print(f\"Mean: {SemanticDF['Semantic_Similarity'].mean():.3f}\")\n",
    "    print(f\"Median: {SemanticDF['Semantic_Similarity'].median():.3f}\")\n",
    "    print(f\"Std Dev: {SemanticDF['Semantic_Similarity'].std():.3f}\")\n",
    "    \n",
    "    print(\"\\nAverage TF-IDF Similarity per Sentence:\")\n",
    "    print(f\"Mean: {sentence_tfidf['Similarity_Ratio'].mean():.3f}\")\n",
    "    print(f\"Median: {sentence_tfidf['Similarity_Ratio'].median():.3f}\")\n",
    "    print(f\"Std Dev: {sentence_tfidf['Similarity_Ratio'].std():.3f}\")\n",
    "    \n",
    "    print(\"\\nROUGE Score:\")\n",
    "    print(f\"Mean: {RougeDF['Rouge_Score'].mean():.3f}\")\n",
    "    print(f\"Median: {RougeDF['Rouge_Score'].median():.3f}\")\n",
    "    print(f\"Std Dev: {RougeDF['Rouge_Score'].std():.3f}\")\n",
    "    \n",
    "    # Calculate correlations between metrics\n",
    "    semantic_tfidf_corr = SemanticDF['Semantic_Similarity'].corr(sentence_tfidf['Similarity_Ratio'])\n",
    "    semantic_rouge_corr = SemanticDF['Semantic_Similarity'].corr(RougeDF['Rouge_Score'])\n",
    "    tfidf_rouge_corr = sentence_tfidf['Similarity_Ratio'].corr(RougeDF['Rouge_Score'])\n",
    "    \n",
    "    print(\"\\nCorrelations between metrics:\")\n",
    "    print(f\"Semantic-TF-IDF: {semantic_tfidf_corr:.3f}\")\n",
    "    print(f\"Semantic-ROUGE: {semantic_rouge_corr:.3f}\")\n",
    "    print(f\"TF-IDF-ROUGE: {tfidf_rouge_corr:.3f}\")\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_sentence_correlation(SemanticDF, TFDF, RougeDF):\n",
    "    \"\"\"Create scatter plot matrix comparing all sentence-level metrics\"\"\"\n",
    "    # Aggregate TF-IDF scores\n",
    "    sentence_tfidf = aggregate_tfidf_to_sentence(TFDF)\n",
    "    \n",
    "    # Sort DataFrames by index to ensure alignment\n",
    "    SemanticDF = SemanticDF.sort_index().reset_index(drop=True)\n",
    "    sentence_tfidf = sentence_tfidf.sort_values('Index').reset_index(drop=True)\n",
    "    RougeDF = RougeDF.sort_index().reset_index(drop=True)\n",
    "    \n",
    "    # Create figure with 2x2 subplots (we'll use 3 of them)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    fig.suptitle('Sentence-Level Metric Correlations', y=1.02, fontsize=16)\n",
    "    \n",
    "    # Semantic vs TF-IDF\n",
    "    axes[0,0].scatter(SemanticDF['Semantic_Similarity'],\n",
    "                     sentence_tfidf['Similarity_Ratio'],\n",
    "                     alpha=0.6, c='purple')\n",
    "    correlation = SemanticDF['Semantic_Similarity'].corr(sentence_tfidf['Similarity_Ratio'])\n",
    "    axes[0,0].set_title(f'Semantic vs TF-IDF (r={correlation:.3f})')\n",
    "    axes[0,0].set_xlabel('Semantic Similarity')\n",
    "    axes[0,0].set_ylabel('TF-IDF Similarity')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Semantic vs ROUGE\n",
    "    axes[0,1].scatter(SemanticDF['Semantic_Similarity'],\n",
    "                     RougeDF['Rouge_Score'],\n",
    "                     alpha=0.6, c='green')\n",
    "    correlation = SemanticDF['Semantic_Similarity'].corr(RougeDF['Rouge_Score'])\n",
    "    axes[0,1].set_title(f'Semantic vs ROUGE (r={correlation:.3f})')\n",
    "    axes[0,1].set_xlabel('Semantic Similarity')\n",
    "    axes[0,1].set_ylabel('ROUGE Score')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # TF-IDF vs ROUGE\n",
    "    axes[1,0].scatter(sentence_tfidf['Similarity_Ratio'],\n",
    "                     RougeDF['Rouge_Score'],\n",
    "                     alpha=0.6, c='blue')\n",
    "    correlation = sentence_tfidf['Similarity_Ratio'].corr(RougeDF['Rouge_Score'])\n",
    "    axes[1,0].set_title(f'TF-IDF vs ROUGE (r={correlation:.3f})')\n",
    "    axes[1,0].set_xlabel('TF-IDF Similarity')\n",
    "    axes[1,0].set_ylabel('ROUGE Score')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove the unused subplot\n",
    "    fig.delaxes(axes[1,1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
