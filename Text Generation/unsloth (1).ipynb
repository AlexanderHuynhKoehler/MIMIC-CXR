{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "CUDA is available!\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA A100-SXM4-80GB\n",
      "Python version: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koehler.ale/.conda/envs/change/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CC'] = os.path.join(os.environ['CONDA_PREFIX'], 'bin', 'x86_64-conda-linux-gnu-gcc')\n",
    "os.environ['CXX'] = os.path.join(os.environ['CONDA_PREFIX'], 'bin', 'x86_64-conda-linux-gnu-g++')\n",
    "import torch\n",
    "print(torch.version.cuda)\n",
    "from PIL import Image\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainerCallback\n",
    "from datasets import Dataset, Features, Array2D\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from transformers import TextStreamer\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # List details for each GPU\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    torch.cuda.empty_cache()  # Clears unused memory\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Delete specific directories\n",
    "# if os.path.exists(\"outputs\"):\n",
    "#     shutil.rmtree(\"outputs\")\n",
    "# if os.path.exists(\"runs\"):\n",
    "#     shutil.rmtree(\"runs\")\n",
    "# del model\n",
    "# import gc\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 MB\n",
      "Cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check current GPU memory usage\n",
    "import torch\n",
    "torch.cuda.memory_summary()\n",
    "\n",
    "# For a simpler view of allocated memory\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0)/1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_single_summary(\n",
    "    image,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    instruction=\"You are an expert radiographer. Describe accurately what you see in this image in detail.\",\n",
    "    max_new_tokens=120,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a summary for a single image using LLaMA.\n",
    "    \n",
    "    Args:\n",
    "        image: The input image\n",
    "        model: The LLaMA model\n",
    "        tokenizer: The LLaMA tokenizer\n",
    "        instruction: The prompt instruction\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        min_p: Minimum probability threshold for sampling\n",
    "        device: Device to run generation on\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated summary text\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction}\n",
    "        ]}\n",
    "    ]\n",
    "    \n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        temperature=temperature,\n",
    "        min_p=min_p\n",
    "    )\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def generate_dataset_summaries(\n",
    "    val_dataset,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    instruction=\"You are an expert radiographer. Describe accurately what you see in this image.\",\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate summaries for all images in a validation dataset and compare with real summaries.\n",
    "    \n",
    "    Args:\n",
    "        val_dataset: The validation dataset containing images and their summaries\n",
    "        model: The LLaMA model\n",
    "        tokenizer: The LLaMA tokenizer\n",
    "        instruction: The prompt instruction\n",
    "        device: Device to run generation on\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with Generated_Summary and Real_Summary columns\n",
    "    \"\"\"\n",
    "    FastVisionModel.for_inference(model)\n",
    "    generated_summaries = []\n",
    "    real_summaries = []\n",
    "    \n",
    "    # Process each item in the dataset\n",
    "    for idx in tqdm(range(len(val_dataset)), desc=\"Generating Summaries\"):\n",
    "        # Get image and real summary from dataset\n",
    "        item = val_dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        real_summary = item[\"Content\"]  # Adjust this based on your dataset's attribute name\n",
    "        \n",
    "        # Generate summary for current image\n",
    "        generated_summary = generate_single_summary(\n",
    "            image=image,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            instruction=instruction,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        generated_summaries.append(generated_summary)\n",
    "        real_summaries.append(real_summary)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Generated_Summary': generated_summaries,\n",
    "        'Real_Summary': real_summaries\n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CONDA_PREFIX: /shared/centos7/anaconda3/2021.05\n",
      "Using GCC at: /home/koehler.ale/.conda/envs/change/bin/x86_64-conda-linux-gnu-gcc\n",
      "Using G++ at: /home/koehler.ale/.conda/envs/change/bin/x86_64-conda-linux-gnu-g++\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print current CONDA_PREFIX to see what it's set to\n",
    "print(f\"Current CONDA_PREFIX: {os.environ.get('CONDA_PREFIX', 'Not set')}\")\n",
    "\n",
    "# We know the compiler exists here from our earlier tests\n",
    "expected_prefix = \"/home/koehler.ale/.conda/envs/change\"\n",
    "\n",
    "# Override CONDA_PREFIX if it's wrong\n",
    "if os.environ.get('CONDA_PREFIX') != expected_prefix:\n",
    "    os.environ['CONDA_PREFIX'] = expected_prefix\n",
    "\n",
    "conda_gcc = os.path.join(os.environ['CONDA_PREFIX'], 'bin', 'x86_64-conda-linux-gnu-gcc')\n",
    "conda_gxx = os.path.join(os.environ['CONDA_PREFIX'], 'bin', 'x86_64-conda-linux-gnu-g++')\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(conda_gcc) or not os.path.exists(conda_gxx):\n",
    "    raise FileNotFoundError(f\"GCC/G++ not found at expected paths: {conda_gcc}, {conda_gxx}\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['CC'] = conda_gcc\n",
    "os.environ['CXX'] = conda_gxx\n",
    "os.environ['GCC_PATH'] = os.path.dirname(conda_gcc)\n",
    "\n",
    "# Print paths to verify\n",
    "print(f\"Using GCC at: {os.environ['CC']}\")\n",
    "print(f\"Using G++ at: {os.environ['CXX']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.10: Fast Mllama vision patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.15 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b6b9440ee54d3096c89390d01bae18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
    "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
    "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
    "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
    "\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
    "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 20,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 20,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /scratch/koehler.ale/xray_dataset\n",
      "Loaded dataset with 2847 examples\n",
      "First image type: <class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "Bilateral hazy opacities interstitial are visualized and likely representative of fibrotic changes. Otherwise the lungs are without a focal consolidation, effusion, or pneumothorax. Cardiomediastinal silhouette is within normal limits. No acute fractures are identified. No evidence of acute injury. Bilateral hazy interstitial opacities are likely representative of fibrotic changes.\n",
      "['Content', 'image']\n",
      "Training set size: 2562\n",
      "Validation set size: 285\n",
      "['Content', 'image']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "def load_scratch_dataset():\n",
    "    \"\"\"\n",
    "    Load the dataset from scratch directory\n",
    "    \n",
    "    Returns:\n",
    "        Dataset: Loaded dataset with PIL images\n",
    "    \"\"\"\n",
    "    load_path = \"/scratch/koehler.ale/xray_dataset\"\n",
    "    \n",
    "    if not os.path.exists(load_path):\n",
    "        raise FileNotFoundError(f\"No dataset found at {load_path}\")\n",
    "    \n",
    "    print(f\"Loading dataset from {load_path}\")\n",
    "    dataset = load_from_disk(load_path)\n",
    "    print(f\"Loaded dataset with {len(dataset)} examples\")\n",
    "    print(f\"First image type: {type(dataset[0]['image'])}\")\n",
    "    print(dataset[0]['Content'])\n",
    "    print(dataset.column_names)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_scratch_dataset()\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Extract the training and validation sets\n",
    "train1_dataset = split_dataset[\"train\"]\n",
    "val1_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Training set size: {len(train1_dataset)}\")\n",
    "print(f\"Validation set size: {len(val1_dataset)}\")\n",
    "print(train1_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"You are an expert radiographer. Describe accurately what you see in this image in detail.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    conversation = [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        { \"role\" : \"assistant\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : sample[\"Content\"]} ]\n",
    "        },\n",
    "    ]\n",
    "    return { \"messages\" : conversation }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [convert_to_conversation(sample) for sample in train1_dataset]\n",
    "val_dataset = [convert_to_conversation(sample) for sample in val1_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    args = SFTConfig(\n",
    "        num_train_epochs = 2,\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 200,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "        report_to=\"none\",\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 10,  \n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-80GB. Max memory = 79.15 GB.\n",
      "21.135 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,562 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 32 | Total steps = 200\n",
      " \"-____-\"     Number of trainable parameters = 83,968,000\n",
      "ðŸ¦¥ Unsloth needs about 1-3 minutes to load everything - please wait!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 29/200 09:20 < 59:09, 0.05 it/s, Epoch 0.35/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.131000</td>\n",
       "      <td>1.001033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.016100</td>\n",
       "      <td>0.901035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "image = val1_dataset[50][\"image\"]\n",
    "instruction = \"You are an expert radiographer. Describe accurately what you see in this image in detail.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainedResults = generate_dataset_summaries(val1_dataset, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, save_path):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        save_path (str): The file path to save the DataFrame (including the filename).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"DataFrame saved to {save_path}\")\n",
    "save_dataframe(UntrainedResults,\"/scratch/koehler.ale/Untrained_DF.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"/scratch/koehler.ale/Untrained_DF.csv\"\n",
    "Untrained_df = load_dataframe(load_path)\n",
    "Untrained_df.rename(columns={\"Generated_Summary\":\"Untrained_Summary\"})\n",
    "Untrained_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalMIMICDF = pd.merge(Untrained_df, TrainedResults, on=\"Real_Summary\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(FinalMIMICDF,\"/scratch/koehler.ale/FinalMIMICDF.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (change)",
   "language": "python",
   "name": "change"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
